{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "from matplotlib import pyplot as plt\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def process(self, state):\n",
    "        output = tf.image.rgb_to_grayscale(state)\n",
    "        output = tf.image.crop_to_bounding_box(output, 34, 0, 160, 160)\n",
    "        output = tf.image.resize(\n",
    "                output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        output = tf.squeeze(output)\n",
    "        return output\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84] state representing grayscale values.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesRegressor():\n",
    "    def __init__(self, lastlayer, num_action,alpha):\n",
    "        self.lastlayer = lastlayer\n",
    "        self.num_action = num_action\n",
    "        self.alpha = alpha\n",
    "        eye = np.zeros((lastlayer,lastlayer))\n",
    "        for i in range(lastlayer):\n",
    "            eye[i,i] = 1\n",
    "        self.E_W = np.random.normal(loc=0, scale=.01, size=(num_action,lastlayer))\n",
    "        self.E_W_target = np.random.normal(loc=0, scale=.01, size=(num_action,lastlayer))\n",
    "        self.E_W_ = np.random.normal(loc=0, scale=.01, size=(num_action,lastlayer))\n",
    "        self.Cov_W = np.random.normal(loc=0, scale= 1, size=(num_action,lastlayer,lastlayer))+eye\n",
    "        self.Cov_W_decom = self.Cov_W\n",
    "        for i in range(num_action):\n",
    "            self.Cov_W[i] = eye\n",
    "            self.Cov_W_decom[i] = np.linalg.cholesky(((self.Cov_W[i]+np.transpose(self.Cov_W[i]))/2.))\n",
    "        self.Cov_W_target = self.Cov_W\n",
    "        self.phiphiT = np.zeros((num_action,lastlayer,lastlayer))\n",
    "        self.phiY = np.zeros((num_action,lastlayer))\n",
    "        \n",
    "    def BayesReg(self,batch_size):\n",
    "        self.phiphiT *= (1-self.alpha) #Forgetting parameter alpha suggest how much of the moment from the past can be used, we set alpha to 1 which means do not use the past moment\n",
    "        self.phiY *= (1-self.alpha)\n",
    "        for j in range(batch_size):\n",
    "            transitions = replay_memory.sample(1) # sample a minibatch of size one from replay buffer\n",
    "            bat_state[0] = transitions[0].state.as_in_context(opt.ctx).astype('float32')/255.\n",
    "            bat_state_next[0] = transitions[0].next_state.as_in_context(opt.ctx).astype('float32')/255.\n",
    "            bat_reward = transitions[0].reward \n",
    "            bat_action = transitions[0].action \n",
    "            bat_done = transitions[0].done \n",
    "            self.phiphiT[int(bat_action)] += nd.dot(dqn_(bat_state).T,dqn_(bat_state))\n",
    "            self.phiY[int(bat_action)] += (dqn_(bat_state)[0].T*(bat_reward +(1.-bat_done) * opt.gamma * nd.max(nd.dot(self.E_W_target,target_dqn_(bat_state_next)[0].T))))\n",
    "            for i in range(num_action):\n",
    "                inv = np.linalg.inv((self.phiphiT[i]/sigma_n + 1/sigma*eye).asnumpy())\n",
    "                self.E_W[i] = nd.array(np.dot(inv,self.phiY[i].asnumpy())/sigma_n, ctx = opt.ctx)\n",
    "                self.Cov_W[i] = sigma * inv\n",
    "                    \n",
    "    def UpdateCov(self):\n",
    "        pass\n",
    "    # Thompson sampling, sample model W form the posterior.\n",
    "    def sample_W(self):\n",
    "        for i in range(self.num_action):\n",
    "            sam = np.random.normal(loc=0, scale=1, size=(self.lastlayer,1))\n",
    "            self.E_W_[i] = self.E_W[i] + np.matmul(bayes_regressor.Cov_W_decom[i],sam)[:,0]\n",
    "        return tf.constant(self.E_W_, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, checkpoint_path=None, summaries_dir=None):\n",
    "        self.tf_global_step = tf.Variable(0)\n",
    "        self.global_step = 0\n",
    "        self.episode = tf.Variable(0)\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        self._build_model()\n",
    "        self.ckpt = None\n",
    "        self.ckm = None\n",
    "        if checkpoint_path:\n",
    "            self.ckpt = tf.train.Checkpoint(optimizer=self.optimizer, \n",
    "                                            model=self.model, \n",
    "                                            tf_global_step=self.tf_global_step,\n",
    "                                            episode = self.episode)\n",
    "            self.ckm = tf.train.CheckpointManager(self.ckpt, checkpoint_path, max_to_keep=3)\n",
    "        if summaries_dir:\n",
    "            summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(name))\n",
    "            if not os.path.exists(summary_dir):\n",
    "                os.makedirs(summary_dir)\n",
    "            self.summary_writer = tf.summary.create_file_writer(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, 8, 4, activation='relu'),\n",
    "            tf.keras.layers.Conv2D(64, 4, 2, activation='relu'),\n",
    "            tf.keras.layers.Conv2D(64, 3, 1, activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(512)#,\n",
    "            #tf.keras.layers.Dense(len(VALID_ACTIONS))\n",
    "        ])\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(0.00025, 0.99, 0.0, 1e-6)\n",
    "\n",
    "\n",
    "    def predict(self, s):\n",
    "        X = s / 255.0\n",
    "        batch_size = tf.shape(X)[0]\n",
    "        predictions = self.model(X)\n",
    "        return predictions\n",
    "    \n",
    "    def save(self):\n",
    "        self.episode.assign_add(1)\n",
    "        if self.ckm:\n",
    "            self.ckm.save()\n",
    "        \n",
    "    def load(self):\n",
    "        if self.ckpt:\n",
    "            self.ckpt.restore(self.ckm.latest_checkpoint)\n",
    "            if self.ckm.latest_checkpoint:\n",
    "                self.global_step = self.tf_global_step.numpy()\n",
    "                print(\"Restored from {}\".format(self.ckm.latest_checkpoint))\n",
    "                print(\"Starting from step {}\".format(self.global_step))\n",
    "            else:\n",
    "                print(\"Initializing from scratch.\")\n",
    "    \n",
    "    def update(self, s, a, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 84, 84, 1]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            X = s / 255.0\n",
    "            batch_size = tf.shape(X)[0]\n",
    "            predictions = self.model(X)\n",
    "            gather_indices = tf.range(batch_size) * tf.shape(predictions)[1] + a\n",
    "            action_predictions = tf.gather(tf.reshape(predictions, [-1]), gather_indices)\n",
    "            # Calculate the loss\n",
    "            losses = tf.math.squared_difference(y, action_predictions)\n",
    "            loss = tf.reduce_mean(losses)\n",
    "            # Optimizer Parameters from original paper\n",
    "        \n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        train_op = self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        self.global_step = self.global_step + 1\n",
    "        if self.ckpt:\n",
    "            self.ckpt.tf_global_step.assign_add(1)\n",
    "        # Summaries for Tensorboard\n",
    "        if self.summary_writer:\n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar(\"loss\", loss,step=self.global_step)\n",
    "                tf.summary.histogram(\"loss_hist\", losses,step=self.global_step)\n",
    "                tf.summary.histogram(\"q_values_hist\", predictions,step=self.global_step)\n",
    "                tf.summary.scalar(\"max_q_value\", tf.reduce_max(predictions),step=self.global_step)\n",
    "                self.summary_writer.flush()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Step: %d %s\" % (step, info))\n",
    "    plt.axis('off')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    bayes_regressor,\n",
    "                    state_processor,                 \n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    total_t = 0,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "        \n",
    "    q_estimator.load()\n",
    "    total_t = q_estimator.global_step\n",
    "    target_estimator.model = tf.keras.models.clone_model(q_estimator.model, input_tensors=None, clone_function=None)\n",
    "    \n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    E_W_ = bayes_regressor.sample_W()\n",
    "    for i in range(replay_memory_init_size):\n",
    "        q_values = q_estimator.predict(np.expand_dims(state, 0))\n",
    "        a = tf.linalg.matvec(E_W_,q_values[0])\n",
    "        action = np.argmax(a.numpy()).astype(np.uint8)\n",
    "        if i % 30 == 0:\n",
    "            action = 1\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        next_state = state_processor.process(next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "    \n",
    "    print(\"Replay memory filled!\")\n",
    "    # Record videos\n",
    "    env= Monitor(env,\n",
    "                 directory=monitor_path,\n",
    "                 resume=True,\n",
    "                 video_callable=lambda count: count % record_video_every == 0)\n",
    "    \n",
    "    for i_episode in range(q_estimator.episode.numpy(), num_episodes):\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "        empty_action_count = 0\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Add epsilon to Tensorboard\n",
    "            with q_estimator.summary_writer.as_default():\n",
    "                tf.summary.histogram(\"epsilon\",epsilon,step=total_t)\n",
    "                q_estimator.summary_writer.flush()\n",
    "\n",
    "            # TODO: Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                target_estimator.model = tf.keras.models.clone_model(q_estimator.model, input_tensors=None, clone_function=None)\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            q_values = q_estimator.predict(np.expand_dims(state, 0))\n",
    "            a = tf.linalg.matvec(E_W_,q_values[0])\n",
    "            action = np.argmax(a.numpy()).astype(np.uint8)\n",
    "            if action != 1:\n",
    "                empty_action_count = empty_action_count + 1\n",
    "                if empty_action_count == 15:\n",
    "                    action = 1\n",
    "                    empty_action_count = 0\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            next_state = state_processor.process(next_state)\n",
    "            #show_state(env, t, \"show\")\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # TODO: Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done)) \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # TODO: Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "            \n",
    "            # Calculate q values and targets\n",
    "            # This is where Double Q-Learning comes in!\n",
    "            q_values_next = q_estimator.predict(next_states_batch)\n",
    "            best_actions = np.argmax(q_values_next, axis=1)\n",
    "            q_values_next_target = target_estimator.predict(next_states_batch)\n",
    "            idx = tf.constant(best_actions, dtype=tf.int32)\n",
    "            row_idx = tf.constant(np.arange(batch_size))\n",
    "            indices = tf.stack([row_idx, idx], axis=1)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * tf.gather_nd(q_values_next_target, indices=indices)\n",
    "\n",
    "            # TODO Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(states_batch, action_batch, targets_batch)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "            \n",
    "        # Save the current checkpoint\n",
    "        q_estimator.save()\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        with q_estimator.summary_writer.as_default():\n",
    "            tf.summary.histogram(\"episode_reward\",stats.episode_rewards[i_episode],step=total_t)\n",
    "            tf.summary.histogram(\"episode_length\",stats.episode_lengths[i_episode],step=total_t)\n",
    "            q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    env.monitor.close()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments_BDQN/{}\".format(env.spec.id))\n",
    "checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "# Create a glboal step variable\n",
    "global_step_q = 0\n",
    "global_step_t = 0\n",
    "alpha = .01\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(name=\"q\",checkpoint_path=checkpoint_dir,summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(name=\"t\",summaries_dir=experiment_dir)\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "bayes_regressor = BayesRegressor(512,4,alpha)\n",
    "# Run it!\n",
    "for t, stats in deep_q_learning(env,\n",
    "                                q_estimator=q_estimator,\n",
    "                                target_estimator=target_estimator,\n",
    "                                bayes_regressor = bayes_regressor,\n",
    "                                state_processor=state_processor,\n",
    "                                experiment_dir=experiment_dir,\n",
    "                                num_episodes=10000,\n",
    "                                replay_memory_size=500000,\n",
    "                                replay_memory_init_size=50000,\n",
    "                                update_target_estimator_every=10000,\n",
    "                                epsilon_start=1.0,\n",
    "                                epsilon_end=0.1,\n",
    "                                epsilon_decay_steps=500000,\n",
    "                                discount_factor=0.99,\n",
    "                                batch_size=32):\n",
    "    print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
